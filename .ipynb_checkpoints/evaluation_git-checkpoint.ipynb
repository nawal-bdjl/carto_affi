{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f43984-1f14-442e-b4d4-26f60062c678",
   "metadata": {},
   "source": [
    "# Evaluation - stage cartographie de l'écosystème\n",
    "\n",
    "Ce notebook est à réaliser chez soi dans un délai de 2 jours, mais rassurez-vous, ça ne prend pas autant de temps ! Il a pour objectif d'évaluer certaines de vos capacités en code et vos bonnes pratiques en scraping, NLP (Natural Language Processing) et manipulation d'API.\n",
    "\n",
    "L'objectif de ce notebook n'est pas de produire des outils opérationnels, mais d'évaluer vos compétences ou votre capacité à chercher des informations.\n",
    "\n",
    "En cas de doute ou de question, n'hésitez pas à nous écrire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601cb69",
   "metadata": {},
   "source": [
    "# 0. Git\n",
    "\n",
    "La première chose à faire est de \"récupérer\" ce notebook pour le compléter. Plusieurs options selon votre maîtrise de Git et/ou que vous ayez un compte :\n",
    "- fork le repo suivant et le compléter sur un projet perso\n",
    "- vous ajouter au projet, créez votre branch `submission_nomprenom` et push dessus\n",
    "- vous envoyez par mail le notebook à compléter et renvoyer par mail\n",
    "\n",
    "https://github.com/TitouanBlaize/carto_affi/blob/main/evaluation_git.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea047399-f5e2-4c73-888e-aeb969972f5c",
   "metadata": {},
   "source": [
    "## 1. NLP\n",
    "\n",
    "Cette première partie évalue vos connaissances des packages de NLP et vos bonnes pratiques quant au preprocessing de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5929e7fb-77d9-4eb6-aae0-e87407b36678",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# écrivez ici une liste de librairies que vous connaissez pour faire du NLP\n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7a5a7-b127-40b1-a502-a027b91f25d7",
   "metadata": {},
   "source": [
    "Dans une première sous-partie, nous vous proposons de coder un pipeline/fonction de preprocessing de données textuelles. \n",
    "\n",
    "L'objectif est, à partir de textes bruts, de produire des textes propres (ponctuation, casse, pluriel, enlever les stopwords,...) qui peuvent être utilisés en entrée d'un pipeline d'apprentissage/fonction. \n",
    "Libre à vous d'inclure et d'implémenter toutes les fonctions de nettoyage que vous souhaitez.\n",
    "Ces opérations peuvent être menées au sein d'une classe (cf class NLPPipeline) ou d'une fonction (NLP_cleaning), comme vous préférez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fef2263-ac24-4943-8e30-7c7049f85f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "class NLPPipeline():\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def transform(self, x):        # Tokenization\n",
    "        tokens = word_tokenize(x)\n",
    "\n",
    "        # Lowercasing\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "\n",
    "        # Removing punctuation\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [token.translate(table) for token in tokens]\n",
    "\n",
    "        # Removing stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Joining tokens back into string\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "313ff353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via une fonction\n",
    "def NLP_cleaning():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54064e06-0c87-465d-bfe8-80a1f5af35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'george washington  1er président usa  habite washington '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = NLPPipeline()\n",
    "pipeline.transform(\"Georges Washington, 1er président des USA, habite à Washington.\")\n",
    "# Ou NLP_cleaning(\"Georges Washington, 1er président des USA, habite à Washington.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b90180",
   "metadata": {},
   "source": [
    "On pourra par exemple essayer d'obtenir `georges washington er president usa habite a washington`, il n'y a pas de \"bon\" résultat, libre à vous d'ajouter plusieurs cleaning de textes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c342679-5a98-4e43-9260-deb132c6ba10",
   "metadata": {},
   "source": [
    "## 2. Scraping/API\n",
    "\n",
    "Dans cette deuxième partie, nous vous proposons d'écrire une petite fonction de scraping.\n",
    "\n",
    "A l'aide de la librairie de votre choix, écrivez une fonction qui permet de trouver le nombre de citations d'un article sur Pubmed.   \n",
    "Vous appliquerez la fonction à l'article suivant: \"New onset refractory status epilepticus (NORSE) *C. Sculier, N. Gaspard*, 2019\" dont l'ID Pubmed est 30482654\n",
    "\n",
    "https://pubmed.ncbi.nlm.nih.gov/30482654/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "990bfcbf-7531-4b5d-98c4-df5610e2d206",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1616955229.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ... import ...\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from ... import ...\n",
    "import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01bf9f-ea70-4a29-be37-3185b5eabf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_citations(pubmed_id):\n",
    "    \"\"\"\n",
    "    get the number of citations of a given pubmed article\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb972d2",
   "metadata": {},
   "source": [
    "En runnant la case suivante on est censé obtenir 19, bonne chance !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67337264-c9ad-4fed-85f2-f7ce96cb35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_citations(30482654)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

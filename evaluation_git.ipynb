{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f43984-1f14-442e-b4d4-26f60062c678",
   "metadata": {},
   "source": [
    "# Evaluation - stage cartographie de l'écosystème\n",
    "\n",
    "Ce notebook est à réaliser chez soi dans un délai de 2 jours, mais rassurez-vous, ça ne prend pas autant de temps ! Il a pour objectif d'évaluer certaines de vos capacités en code et vos bonnes pratiques en scraping, NLP (Natural Language Processing) et manipulation d'API.\n",
    "\n",
    "L'objectif de ce notebook n'est pas de produire des outils opérationnels, mais d'évaluer vos compétences ou votre capacité à chercher des informations.\n",
    "\n",
    "En cas de doute ou de question, n'hésitez pas à nous écrire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601cb69",
   "metadata": {},
   "source": [
    "# 0. Git\n",
    "\n",
    "La première chose à faire est de \"récupérer\" ce notebook pour le compléter. Plusieurs options selon votre maîtrise de Git et/ou que vous ayez un compte :\n",
    "- fork le repo suivant et le compléter sur un projet perso\n",
    "- vous ajouter au projet, créez votre branch `submission_nomprenom` et push dessus\n",
    "- vous envoyez par mail le notebook à compléter et renvoyer par mail\n",
    "\n",
    "https://github.com/TitouanBlaize/carto_affi/blob/main/evaluation_git.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea047399-f5e2-4c73-888e-aeb969972f5c",
   "metadata": {},
   "source": [
    "## 1. NLP\n",
    "\n",
    "Cette première partie évalue vos connaissances des packages de NLP et vos bonnes pratiques quant au preprocessing de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5929e7fb-77d9-4eb6-aae0-e87407b36678",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# écrivez ici une liste de librairies que vous connaissez pour faire du NLP\n",
    "#import nltk\n",
    "#import spacy\n",
    "#import gensim\n",
    "#import textblob\n",
    "#import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7a5a7-b127-40b1-a502-a027b91f25d7",
   "metadata": {},
   "source": [
    "Dans une première sous-partie, nous vous proposons de coder un pipeline/fonction de preprocessing de données textuelles. \n",
    "\n",
    "L'objectif est, à partir de textes bruts, de produire des textes propres (ponctuation, casse, pluriel, enlever les stopwords,...) qui peuvent être utilisés en entrée d'un pipeline d'apprentissage/fonction. \n",
    "Libre à vous d'inclure et d'implémenter toutes les fonctions de nettoyage que vous souhaitez.\n",
    "Ces opérations peuvent être menées au sein d'une classe (cf class NLPPipeline) ou d'une fonction (NLP_cleaning), comme vous préférez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "2fef2263-ac24-4943-8e30-7c7049f85f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via une classe (\"Orienté objet\")\n",
    "\n",
    "#importation des librairies nécessaires\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class NLPPipeline():\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('stopwords')\n",
    "        self.lemmatizer = WordNetLemmatizer() #Récupération du lemmatiseur\n",
    "        self.stop_words = set(stopwords.words('french')) #Récupération des stopwords\n",
    "\n",
    "    def transform(self, x):\n",
    "\n",
    "        #Transformation du texte en minuscule\n",
    "        textLower = x.lower()\n",
    "\n",
    "        #Suppression des chiffres ainsi que des signes de ponctuation\n",
    "        textCleaned = \"\"\n",
    "        for c in textLower:\n",
    "            if not c.isdigit() and c not in string.punctuation:\n",
    "                textCleaned += c\n",
    "\n",
    "        #Tokenization\n",
    "        tokens = word_tokenize(textCleaned, language='french')\n",
    "\n",
    "        #Lemmatization\n",
    "        lemmatizedTokens = []\n",
    "        for token in tokens:\n",
    "            lemmatizedTokens.append(self.lemmatizer.lemmatize(token))\n",
    "\n",
    "        #Suppression des stopwords\n",
    "        filteredTokens = []\n",
    "        for token in lemmatizedTokens:\n",
    "            if token not in self.stop_words:\n",
    "                filteredTokens.append(token)\n",
    "\n",
    "        #Reconstruction du texte\n",
    "        cleanedText = ' '.join(filteredTokens)\n",
    "\n",
    "        return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "313ff353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Via une fonction\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les données nécessaires de nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def NLP_cleaning(x):\n",
    "    lemmatizer = WordNetLemmatizer() #lemmatiseur\n",
    "    stop_words = set(stopwords.words('french')) # stopwords\n",
    "\n",
    "    #Transformation du texte en minuscule\n",
    "    text_lower = x.lower()\n",
    "\n",
    "    #Suppression de la ponctuation et des chiffres\n",
    "    text_cleaned = \"\"\n",
    "    for c in text_lower:\n",
    "        if not c.isdigit() and c not in string.punctuation:\n",
    "            text_cleaned += c\n",
    "\n",
    "    #Tokenization\n",
    "    tokens = word_tokenize(text_cleaned, language='french')\n",
    "\n",
    "    #Lemmatization\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    #Suppression stopwords\n",
    "    filtered_tokens = []\n",
    "    for token in lemmatized_tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    #Reconstruction du texte\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "54064e06-0c87-465d-bfe8-80a1f5af35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nawalbendjelloul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "'george washington er président usa habite washington'"
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = NLPPipeline()\n",
    "pipeline.transform(\"Georges Washington, 1er président des USA, habite à Washington.\")\n",
    "NLP_cleaning(\"Georges Washington, 1er président des USA, habite à Washington.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b90180",
   "metadata": {},
   "source": [
    "On pourra par exemple essayer d'obtenir `georges washington er president usa habite a washington`, il n'y a pas de \"bon\" résultat, libre à vous d'ajouter plusieurs cleaning de textes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c342679-5a98-4e43-9260-deb132c6ba10",
   "metadata": {},
   "source": [
    "## 2. Scraping/API\n",
    "\n",
    "Dans cette deuxième partie, nous vous proposons d'écrire une petite fonction de scraping.\n",
    "\n",
    "A l'aide de la librairie de votre choix, écrivez une fonction qui permet de trouver le nombre de citations d'un article sur Pubmed.   \n",
    "Vous appliquerez la fonction à l'article suivant: \"New onset refractory status epilepticus (NORSE) *C. Sculier, N. Gaspard*, 2019\" dont l'ID Pubmed est 30482654\n",
    "\n",
    "https://pubmed.ncbi.nlm.nih.gov/30482654/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "990bfcbf-7531-4b5d-98c4-df5610e2d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_number_of_citations(pubmed_id):\n",
    "    #Création l'URL de l'API Europe PMC  avec l'ID Pubmed\n",
    "    url = f'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:{pubmed_id}&format=json'\n",
    "\n",
    "    # Envoi d'une requête GET à l'URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Vérification de l'exécution correcte de la requête\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        # Extraction des données JSON de la réponse\n",
    "        data = response.json()\n",
    "\n",
    "        # Vérification de l'envoi des réponses\n",
    "        if data.get(\"resultList\") and data[\"resultList\"].get(\"result\"):\n",
    "\n",
    "            # Extraction du nombre de citations du premier résultat\n",
    "            citation_count = data[\"resultList\"][\"result\"][0].get(\"citedByCount\")\n",
    "\n",
    "            # Vérification de l'existence du nombre de citations\n",
    "            if citation_count is not None:\n",
    "                return int(citation_count)\n",
    "            else:\n",
    "                return \"Citation count not found.\"\n",
    "        else:\n",
    "            return \"Article not found.\"\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: Could not fetch the URL.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "4cb972d2",
   "metadata": {},
   "source": [
    "En runnant la case suivante, on est censé obtenir 19, bonne chance !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "67337264-c9ad-4fed-85f2-f7ce96cb35ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(get_number_of_citations(30482654))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
